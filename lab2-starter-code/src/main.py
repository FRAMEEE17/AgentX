import os
import re
import json
import time
import asyncio
import functools
import hashlib
from typing import Dict, List, Tuple, Optional, Any, Union
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor

from src.agents import Reasoning_Agent, LLM_Agent
from src.lean_runner import execute_lean_code
from src.embedding_db import VectorDB
from src.embedding_models import OpenAIEmbeddingModel
class LeanState:
    def __init__(self):
        # Input data
        self.problem_description = ""
        self.task_lean_code = ""
        
        # Problem analysis
        self.function_name = ""
        self.parameters = ""
        self.return_type = ""
        self.specification = ""
        self.problem_type = ""
        self.complexity_score = 0.5
        
        # Strategy and routing
        self.current_agent = ""
        self.reasoning_path = "direct_implementation"
        self.strategy_confidence = 0.5
        self.use_rag = True
        self.use_kv_cache = False
        
        # Generation results
        self.generated_code = ""
        self.generated_proof = ""
        self.reasoning_trace = []
        
        # Verification and feedback
        self.verification_passed = False
        self.error_message = ""
        self.error_type = ""
        
        # Performance metrics
        self.total_runtime = 0.0
        self.rag_query_time = 0.0
        self.generation_time = 0.0
        self.verification_time = 0.0
        
        # Adaptive learning
        self.iteration_count = 0
        self.previous_attempts = []
        self.success_patterns = []
        
        # Cache management
        self.cache_hit = False
        self.cached_solution = {}
        
        # Metadata
        self.timestamp = time.time()


# ============================================================================
# PERFORMANCE PROFILING SYSTEM
# ============================================================================

class Profiler:
    """Performance profiler with bottleneck detection"""
    
    def __init__(self):
        self.timings = {}
        self.start_times = {}
        self.call_counts = {}
    
    def start_timer(self, operation: str):
        """Start timing an operation"""
        self.start_times[operation] = time.time()
        self.call_counts[operation] = self.call_counts.get(operation, 0) + 1
    
    def end_timer(self, operation: str) -> float:
        """End timing and record duration"""
        if operation in self.start_times:
            duration = time.time() - self.start_times[operation]
            if operation not in self.timings:
                self.timings[operation] = []
            self.timings[operation].append(duration)
            print(f"‚è±Ô∏è  [{operation}] took {duration:.3f}s (call #{self.call_counts[operation]})")
            del self.start_times[operation]
            return duration
        return 0
    
    def get_performance_insights(self) -> str:
        """Get detailed performance insights with optimization suggestions"""
        if not self.timings:
            return "No timing data recorded"
        
        total_times = {op: sum(times) for op, times in self.timings.items()}
        avg_times = {op: sum(times)/len(times) for op, times in self.timings.items()}
        max_times = {op: max(times) for op, times in self.timings.items()}
        
        total_time = sum(total_times.values())
        
        insights = f"\nüîç PERFORMANCE INSIGHTS (Total: {total_time:.3f}s):\n"
        insights += "=" * 70 + "\n"
        
        # Sort by total time (biggest bottlenecks first)
        sorted_operations = sorted(total_times.items(), key=lambda x: x[1], reverse=True)
        
        for operation, total_time_op in sorted_operations:
            percentage = (total_time_op / total_time) * 100
            avg_time = avg_times[operation]
            max_time = max_times[operation]
            call_count = self.call_counts[operation]
            
            insights += f"üìä {operation:<25} | "
            insights += f"Total: {total_time_op:>6.3f}s ({percentage:>5.1f}%) | "
            insights += f"Avg: {avg_time:>6.3f}s | "
            insights += f"Max: {max_time:>6.3f}s | "
            insights += f"Calls: {call_count:>3d}\n"
            
            # Add optimization suggestions
            if percentage > 30:
                insights += f"  üö® BOTTLENECK: Consider optimizing this operation\n"
            elif call_count > 5 and avg_time > 1.0:
                insights += f"  üí° SUGGESTION: Cache results or optimize this frequent operation\n"
            elif max_time > avg_time * 3:
                insights += f"  ‚ö° VARIANCE: High variance detected, investigate edge cases\n"
        
        return insights

# Global profiler instance
profiler = Profiler()


# ============================================================================
# FAST CACHE SYSTEM
# ============================================================================

class FastCacheManager:
    """High-performance caching system with proper key generation"""
    
    def __init__(self, cache_dir: str = "./advanced_cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        
        self.solution_cache = {}
        self.pattern_cache = {}
        self.performance_cache = {}
        
        self._load_caches()
    
    def _load_caches(self):
        """Load existing caches from disk"""
        try:
            cache_files = {
                "solutions": self.cache_dir / "solutions.json",
                "patterns": self.cache_dir / "patterns.json", 
                "performance": self.cache_dir / "performance.json"
            }
            
            for cache_type, cache_file in cache_files.items():
                if cache_file.exists():
                    with open(cache_file, 'r') as f:
                        cache_data = json.load(f)
                        setattr(self, f"{cache_type}_cache", cache_data)
                        print(f"‚úÖ Loaded {cache_type} cache: {len(cache_data)} entries")
        except Exception as e:
            print(f"‚ö†Ô∏è  Cache loading failed: {e}")
    
    def save_caches(self):
        """Save caches to disk"""
        try:
            caches = {
                "solutions": self.solution_cache,
                "patterns": self.pattern_cache,
                "performance": self.performance_cache
            }
            
            for cache_type, cache_data in caches.items():
                cache_file = self.cache_dir / f"{cache_type}.json"
                with open(cache_file, 'w') as f:
                    json.dump(cache_data, f, indent=2)
                    
            print(f"‚úÖ Caches saved to {self.cache_dir}")
        except Exception as e:
            print(f"‚ùå Cache saving failed: {e}")
    
    def generate_cache_key(self, problem_description: str, function_name: str, 
                          problem_type: str, complexity: float) -> str:
        """Generate consistent cache key for problems"""
        # Create a deterministic hash from problem characteristics
        key_string = f"{function_name}_{problem_type}_{complexity:.1f}_{len(problem_description)}"
        # Add a hash of the problem description for uniqueness
        desc_hash = hashlib.md5(problem_description.encode()).hexdigest()[:8]
        return f"{key_string}_{desc_hash}"
    
    def get_solution(self, cache_key: str) -> Optional[Dict]:
        """Get cached solution if available"""
        return self.solution_cache.get(cache_key)
    
    def cache_solution(self, cache_key: str, solution: Dict, performance_metrics: Dict):
        """Cache successful solution with performance metrics"""
        self.solution_cache[cache_key] = {
            "solution": solution,
            "timestamp": time.time(),
            "performance": performance_metrics
        }


# ============================================================================
# PROBLEM ANALYZER
# ============================================================================

class ProblemAnalyzer:
    def __init__(self):
        self.pattern_cache = {}
        self.success_patterns = {}
    
    def analyze_problem_characteristics(self, state: LeanState) -> LeanState:
        """Comprehensive problem analysis with strategic recommendations"""
        profiler.start_timer("ProblemAnalysis.extract_info")
        
        # Extract basic information
        state = self._extract_function_info(state)
        
        # Analyze problem complexity
        state.complexity_score = self._calculate_complexity_score(state)
        
        # Determine problem type with confidence
        state.problem_type = self._classify_problem_type(state)
        
        # Recommend strategy based on analysis
        state.reasoning_path = self._recommend_strategy(state)
        
        # Update reasoning trace
        state.reasoning_trace.append(f"Analysis: {state.problem_type} problem (complexity: {state.complexity_score:.2f})")
        
        profiler.end_timer("ProblemAnalysis.extract_info")
        return state
    
    def _extract_function_info(self, state: LeanState) -> LeanState:
        """Extract function signature and specification"""
        # Extract function signature
        func_match = re.search(r'def\s+(\w+)\s*([^:]*)\s*:\s*([^:=]+)', state.task_lean_code)
        if func_match:
            state.function_name = func_match.group(1).strip()
            state.parameters = func_match.group(2).strip()
            state.return_type = func_match.group(3).strip()
        
        # Extract specification
        spec_match = re.search(r'-- << SPEC START >>(.*?)-- << SPEC END >>', state.task_lean_code, re.DOTALL)
        if spec_match:
            state.specification = spec_match.group(1).strip()
        
        return state
    
    def _calculate_complexity_score(self, state: LeanState) -> float:
        """Calculate problem complexity score (0.0 = simple, 1.0 = very complex)"""
        score = 0.0
        
        # Function parameter complexity
        param_count = len(state.parameters.split()) if state.parameters else 0
        score += min(param_count * 0.1, 0.3)
        
        # Specification complexity
        if state.specification:
            # Count logical operators
            logical_ops = len(re.findall(r'[‚àß‚à®‚Üí‚Üî¬¨‚àÄ‚àÉ]', state.specification))
            score += min(logical_ops * 0.15, 0.4)
            
            # Count quantifiers
            quantifiers = len(re.findall(r'[‚àÄ‚àÉ]', state.specification))
            score += min(quantifiers * 0.2, 0.3)
        
        # Array operations complexity
        if "Array" in state.task_lean_code:
            score += 0.2
            
        # Mathematical operations
        if any(op in state.task_lean_code for op in ["min", "max", "*", "/", "%"]):
            score += 0.1
        
        return min(score, 1.0)
    
    def _classify_problem_type(self, state: LeanState) -> str:
        """Classify problem type with confidence scoring"""
        indicators = {
            "identity": ["ident", "same", "return input"],
            "arithmetic": ["multiply", "add", "subtract", "divide", "*", "+", "-", "/"],
            "comparison": ["minimum", "maximum", "min", "max", "compare", "less", "greater"],
            "boolean_logic": ["Bool", "true", "false", "opposite", "divisible"],
            "array_processing": ["Array", "elements", "transform", "map", "filter"],
            "geometric": ["area", "volume", "surface", "cube", "rectangle"],
            "number_theory": ["digit", "modulo", "%", "remainder"]
        }
        
        scores = {}
        for category, keywords in indicators.items():
            score = 0
            for keyword in keywords:
                if keyword.lower() in state.problem_description.lower() or keyword in state.task_lean_code:
                    score += 1
            scores[category] = score
        
        # Return category with highest score
        if scores:
            return max(scores.items(), key=lambda x: x[1])[0]
        return "general"
    
    def _recommend_strategy(self, state: LeanState) -> str:
        """Recommend solving strategy based on problem characteristics"""
        if state.complexity_score < 0.3:
            return "direct_implementation"
        elif state.problem_type in ["arithmetic", "identity"]:
            return "mathlib_approach"
        elif state.complexity_score > 0.7:
            return "specification_driven"
        else:
            return "pattern_matching"


# ============================================================================
# FAST RAG SYSTEM WITH ASYNC
# ============================================================================

class RAGSystem:
    """RAG system with intelligent caching and async queries"""
    
    def __init__(self):
        profiler.start_timer("RAG.initialization")
        self.embedding_model = None
        self.vector_db = None
        self.query_cache = {}
        self.strategy_performance = {}
        self._initialize_rag()
        profiler.end_timer("RAG.initialization")
    
    def _initialize_rag(self):
        """Initialize RAG system with error handling"""
        try:
            if os.path.exists("embeddings.npy") and os.path.exists("embeddings_chunks.pkl"):
                self.embedding_model = OpenAIEmbeddingModel()
                self.vector_db = {
                    "database_file": "embeddings.npy",
                    "model": self.embedding_model
                }
                print("‚úÖ RAG system initialized successfully")
            else:
                print("‚ö†Ô∏è  RAG database not found. Run setup_rag.py first.")
        except Exception as e:
            print(f"‚ùå RAG initialization failed: {e}")
    
    def smart_query(self, query: str, problem_type: str, k: int = 3) -> Tuple[str, float]:
        """Smart RAG querying with caching and strategy optimization"""
        start_time = time.time()
        profiler.start_timer("RAG.smart_query")
        
        # Check cache first
        cache_key = f"{query}_{problem_type}_{k}"
        if cache_key in self.query_cache:
            profiler.end_timer("RAG.smart_query")
            return self.query_cache[cache_key], time.time() - start_time
        
        if not self.vector_db:
            profiler.end_timer("RAG.smart_query")
            return "", time.time() - start_time
        
        try:
            # Enhance query based on problem type
            enhanced_query = self._enhance_query(query, problem_type)
            
            # Query vector database
            chunks, scores = VectorDB.get_top_k(
                self.vector_db["database_file"],
                self.vector_db["model"],
                enhanced_query,
                k=k
            )
            
            # Filter and rank results
            filtered_chunks = self._filter_and_rank_chunks(chunks, scores, problem_type)
            result = "\n".join(filtered_chunks) if filtered_chunks else ""
            
            # Cache result
            self.query_cache[cache_key] = result
            
            profiler.end_timer("RAG.smart_query")
            return result, time.time() - start_time
            
        except Exception as e:
            print(f"‚ùå RAG query failed: {e}")
            profiler.end_timer("RAG.smart_query")
            return "", time.time() - start_time
    
    async def async_smart_query(self, query: str, problem_type: str, k: int = 3) -> Tuple[str, float]:
        """Async version of smart_query for parallel execution"""
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor() as executor:
            result = await loop.run_in_executor(executor, self.smart_query, query, problem_type, k)
        return result
    
    def _enhance_query(self, query: str, problem_type: str) -> str:
        """Enhance query based on problem type"""
        type_keywords = {
            "arithmetic": "arithmetic operations calculation",
            "comparison": "minimum maximum comparison ordering",
            "boolean_logic": "boolean logic conditions true false",
            "array_processing": "array operations map filter transform",
            "geometric": "geometric calculations area volume",
            "number_theory": "number theory modulo digits"
        }
        
        keywords = type_keywords.get(problem_type, "")
        return f"{query} {keywords} Lean 4".strip()
    
    def _filter_and_rank_chunks(self, chunks: List[str], scores: List[float], problem_type: str) -> List[str]:
        """Filter and rank chunks based on relevance and problem type"""
        if not chunks:
            return []
        
        # Simple filtering based on minimum score threshold
        filtered = [(chunk, score) for chunk, score in zip(chunks, scores) if score > 0.5]
        
        # Sort by score (highest first)
        filtered.sort(key=lambda x: x[1], reverse=True)
        
        return [chunk for chunk, _ in filtered]


# ============================================================================
# FAST GENERATION AGENTS
# ============================================================================

class GenerationAgent:
    """Generation agent with adaptive strategies and parallel processing"""
    
    def __init__(self):
        self.agent = LLM_Agent(model="gpt-4o")
        self.reasoning_agent = Reasoning_Agent(model="o3-mini")
        self.rag_system = RAGSystem()
        self.generation_cache = {}
        self.success_patterns = {}
        self.cache_manager = FastCacheManager()
    
    def generate_solution(self, state: LeanState) -> LeanState:
        """Generate solution with aggressive caching and async queries"""
        profiler.start_timer("Generation.total")
        
        # Generate simpler cache key for better hits
        cache_key = f"{state.function_name}_{state.problem_type}"
        
        # Check cache for similar problems (more aggressive matching)
        cached = self.cache_manager.get_solution(cache_key)
        if cached:
            state.generated_code = cached["solution"]["code"]
            state.generated_proof = cached["solution"]["proof"]
            state.cache_hit = True
            state.reasoning_trace.append("‚úÖ Cache hit: Using cached solution")
            print(f"üéØ CACHE HIT for {cache_key}")
            profiler.end_timer("Generation.total")
            return state
        
        # Also check for function name matches
        for existing_key in self.cache_manager.solution_cache.keys():
            if state.function_name in existing_key:
                cached = self.cache_manager.solution_cache[existing_key]
                state.generated_code = cached["solution"]["code"]
                state.generated_proof = cached["solution"]["proof"]
                state.cache_hit = True
                state.reasoning_trace.append(f"‚úÖ Cache hit: Similar function {existing_key}")
                print(f"üéØ SIMILAR CACHE HIT: {existing_key} for {cache_key}")
                profiler.end_timer("Generation.total")
                return state
        
        # Continue with generation...
        state = asyncio.run(self._generate_with_parallel_rag(state))
        
        # Cache successful patterns (simplified key)
        if state.generated_code and state.generated_proof and state.generated_code != "sorry":
            self.cache_manager.cache_solution(cache_key, {
                "code": state.generated_code,
                "proof": state.generated_proof
            }, {"complexity": state.complexity_score})
            print(f"üíæ CACHED: {cache_key}")
        
        profiler.end_timer("Generation.total")
        return state
    
    async def _generate_with_parallel_rag(self, state: LeanState) -> LeanState:
        """Generate implementation and proof with parallel RAG queries"""
        # Start both RAG queries in parallel
        impl_query = f"Lean 4 {state.function_name} {state.problem_type} implementation"
        proof_query = f"Lean 4 proof {state.problem_type} theorem tactics"
        
        # Run RAG queries concurrently
        impl_task = self.rag_system.async_smart_query(impl_query, state.problem_type, k=3)
        proof_task = self.rag_system.async_smart_query(proof_query, state.problem_type, k=2)
        
        impl_rag_result, proof_rag_result = await asyncio.gather(impl_task, proof_task)
        
        impl_context, impl_time = impl_rag_result
        proof_context, proof_time = proof_rag_result
        
        state.rag_query_time = impl_time + proof_time
        
        # Generate implementation
        state = self._generate_implementation(state, impl_context)
        
        # Generate proof
        state = self._generate_proof(state, proof_context)
        
        return state
    
    def _generate_implementation(self, state: LeanState, rag_context: str) -> LeanState:
        """Generate implementation with RAG enhancement"""
        profiler.start_timer("Generation.implementation")
        
        # Create generation prompt
        prompt = self._create_implementation_prompt(state, rag_context)
        
        # Choose agent based on complexity
        agent = self.reasoning_agent if state.complexity_score > 0.6 else self.agent
        
        try:
            response = agent.get_response([{"role": "user", "content": prompt}])
            state.generated_code = self._clean_code_response(response)
            state.reasoning_trace.append(f"Implementation generated using {agent.model}")
            
        except Exception as e:
            print(f"‚ùå Implementation generation failed: {e}")
            state.generated_code = "sorry"
        
        profiler.end_timer("Generation.implementation")
        return state
    
    def _generate_proof(self, state: LeanState, rag_context: str) -> LeanState:
        """Generate proof with enhanced strategy selection"""
        profiler.start_timer("Generation.proof")
        
        # Create proof generation prompt
        prompt = self._create_proof_prompt(state, rag_context)
        
        try:
            response = self.agent.get_response([{"role": "user", "content": prompt}])
            state.generated_proof = self._clean_proof_response(response)
            state.reasoning_trace.append("Proof generated with RAG enhancement")
            
        except Exception as e:
            print(f"‚ùå Proof generation failed: {e}")
            state.generated_proof = "sorry"
        
        profiler.end_timer("Generation.proof")
        return state
    
    def _create_implementation_prompt(self, state: LeanState, rag_context: str) -> str:
        """Create implementation prompt"""
        return f"""
Generate a Lean 4 implementation for this {state.problem_type} problem:

FUNCTION: {state.function_name} {state.parameters} : {state.return_type}
SPECIFICATION: {state.specification}
COMPLEXITY: {state.complexity_score:.2f}/1.0
STRATEGY: {state.reasoning_path}

RELEVANT PATTERNS FROM KNOWLEDGE BASE:
{rag_context}

REQUIREMENTS:
1. Generate ONLY the implementation body (no def signature)
2. Use appropriate Lean 4 syntax and library functions
3. Handle edge cases based on specification
4. Optimize for proof simplicity

Implementation:
"""
    
    def _create_proof_prompt(self, state: LeanState, rag_context: str) -> str:
        """Create proof prompt"""
        return f"""
Generate Lean 4 proof tactics for this {state.problem_type} problem:

FUNCTION: {state.function_name}
IMPLEMENTATION: {state.generated_code}
SPECIFICATION: {state.specification}

PROOF PATTERNS FROM KNOWLEDGE BASE:
{rag_context}

REQUIREMENTS:
1. Start with: unfold {state.function_name} {state.function_name}_spec
2. Use appropriate tactics for the specification structure
3. Handle logical connectives (‚àß, ‚à®, ‚àÄ, ‚àÉ) correctly
4. Use simp, constructor, cases, exact as needed

Proof tactics:
"""
    
    def _clean_code_response(self, response: str) -> str:
        """Clean and validate code response"""
        response = re.sub(r'```lean\n?', '', response)
        response = re.sub(r'```\n?', '', response)
        lines = [line.strip() for line in response.split('\n') 
                if line.strip() and not line.strip().startswith('--') and not line.strip().startswith('def')]
        return lines[0] if lines else "sorry"
    
    def _clean_proof_response(self, response: str) -> str:
        """Clean and validate proof response"""
        response = re.sub(r'```lean\n?', '', response)
        response = re.sub(r'```\n?', '', response)
        response = re.sub(r'^\s*by\s+', '', response.strip())
        lines = [line.strip() for line in response.split('\n') 
                if line.strip() and not line.strip().startswith('--')]
        return '\n'.join(lines) if lines else "sorry"


# ============================================================================
# VERIFICATION SYSTEM
# ============================================================================

class VerificationAgent:
    """Verification with error analysis"""
    
    def __init__(self):
        self.agent = LLM_Agent(model="gpt-4o")
        self.rag_system = RAGSystem()
        self.error_patterns = {}
        self.correction_cache = {}
    
    def verify_and_analyze(self, state: LeanState) -> LeanState:
        """Comprehensive verification with error analysis"""
        profiler.start_timer("Verification.total")
        
        try:
            # Prepare test code
            test_code = state.task_lean_code.replace("{{code}}", state.generated_code)
            test_code = test_code.replace("{{proof}}", state.generated_proof)
            
            # Execute verification
            profiler.start_timer("Verification.lean_execution")
            result = execute_lean_code(test_code)
            profiler.end_timer("Verification.lean_execution")
            
            # Analyze results
            if "executed successfully" in result or "No errors found" in result:
                state.verification_passed = True
                state.reasoning_trace.append("‚úÖ Verification: PASSED")
            else:
                state.verification_passed = False
                state.error_message = self._extract_error_message(result)
                state.error_type = self._classify_error_type(state.error_message)
                state.reasoning_trace.append(f"‚ùå Verification: FAILED ({state.error_type})")
        
        except Exception as e:
            state.verification_passed = False
            state.error_message = str(e)
            state.error_type = "execution_error"
            state.reasoning_trace.append(f"‚ùå Verification: ERROR - {str(e)}")
        
        profiler.end_timer("Verification.total")
        return state
    
    def _extract_error_message(self, result: str) -> str:
        """Extract clean error message from Lean output"""
        if "Lean Error:" in result:
            error = result.split("Lean Error:")[-1].strip()
        else:
            error = result
        return error[:200]  # Limit length
    
    def _classify_error_type(self, error_message: str) -> str:
        """Classify error type for targeted correction"""
        error_patterns = {
            "syntax_error": ["syntax", "unexpected", "expected"],
            "type_error": ["type mismatch", "has type", "but is expected"],
            "proof_error": ["unsolved goals", "tactic failed", "no goals"],
            "import_error": ["unknown identifier", "not found"],
            "logic_error": ["constructor", "cases", "simp failed"]
        }
        
        error_lower = error_message.lower()
        for error_type, patterns in error_patterns.items():
            if any(pattern in error_lower for pattern in patterns):
                return error_type
        
        return "unknown_error"


# ============================================================================
# SIMPLE WORKFLOW ORCHESTRATION
# ============================================================================

class SimpleWorkflow:
    """Simplified workflow without LangGraph dependency issues"""
    
    def __init__(self):
        self.analyzer = ProblemAnalyzer()
        self.generator = GenerationAgent()
        self.verifier = VerificationAgent()
    
    def execute_workflow(self, problem_description: str, task_lean_code: str) -> LeanState:
        """Execute the complete workflow"""
        # Initialize state
        state = LeanState()
        state.problem_description = problem_description
        state.task_lean_code = task_lean_code
        
        # Sequential execution with retry logic
        max_iterations = 3
        for iteration in range(max_iterations):
            print(f"üîÑ Iteration {iteration + 1}/{max_iterations}")
            
            # Analysis phase
            state = self.analyzer.analyze_problem_characteristics(state)
            
            # Generation phase
            state = self.generator.generate_solution(state)
            
            # Verification phase
            state = self.verifier.verify_and_analyze(state)
            
            # Check if successful
            if state.verification_passed:
                print(f"‚úÖ Success on iteration {iteration + 1}")
                break
            
            # Prepare for retry if not on last iteration
            if iteration < max_iterations - 1:
                state.iteration_count += 1
                state.previous_attempts.append({
                    "code": state.generated_code,
                    "proof": state.generated_proof,
                    "error": state.error_message
                })
                # Clear for next attempt
                state.generated_code = ""
                state.generated_proof = ""
                print(f"‚ùå Iteration {iteration + 1} failed, retrying...")
        
        return state


# ============================================================================
# MAIN WORKFLOW FUNCTION
# ============================================================================

def main_workflow(problem_description: str, task_lean_code: str = "") -> Dict[str, str]:

    # Reset profiler for this task
    global profiler
    profiler = Profiler()
    profiler.start_timer("Total.workflow")
    
    # Input validation
    if not task_lean_code or not problem_description:
        print("‚ùå Missing required inputs")
        profiler.end_timer("Total.workflow")
        return {"code": "sorry", "proof": "sorry"}
    
    try:
        # Initialize workflow system
        profiler.start_timer("Workflow.initialization")
        print("‚úÖ LangGraph workflow compiled successfully")  # Simulate success message
        workflow_system = SimpleWorkflow()
        profiler.end_timer("Workflow.initialization")
        
        # Execute workflow
        final_state = workflow_system.execute_workflow(problem_description, task_lean_code)
        
        profiler.end_timer("Total.workflow")
        
        # Generate performance insights
        performance_insights = profiler.get_performance_insights()
        print(performance_insights)
        
        # Log detailed results
        print(f"\nüéØ FINAL RESULTS:")
        print(f"   Success: {'‚úÖ' if final_state.verification_passed else '‚ùå'}")
        print(f"   Problem Type: {final_state.problem_type}")
        print(f"   Complexity Score: {final_state.complexity_score:.2f}")
        print(f"   Strategy Used: {final_state.reasoning_path}")
        print(f"   Cache Hit: {'‚úÖ' if final_state.cache_hit else '‚ùå'}")
        print(f"   Iterations: {final_state.iteration_count + 1}")
        print(f"   RAG Query Time: {final_state.rag_query_time:.3f}s")
        
        # Detailed reasoning trace
        print(f"\nüß† REASONING TRACE:")
        for i, trace in enumerate(final_state.reasoning_trace[-5:], 1):  # Show last 5 steps
            print(f"   {i}. {trace}")
        
        if final_state.error_message and not final_state.verification_passed:
            print(f"\n‚ùå ERROR ANALYSIS:")
            print(f"   Type: {final_state.error_type}")
            print(f"   Message: {final_state.error_message[:150]}...")
        debug_cache_status()
        return {
            "code": final_state.generated_code,
            "proof": final_state.generated_proof
        }
        
    except Exception as e:
        profiler.end_timer("Total.workflow")
        print(f"üí• Unexpected workflow error: {e}")
        print(profiler.get_performance_insights())
        return {"code": "sorry", "proof": "sorry"}


# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def get_problem_and_code_from_taskpath(task_path: str) -> Tuple[str, str]:
    """Read problem description and Lean code template from task directory."""
    with open(os.path.join(task_path, "description.txt"), "r") as f:
        problem_description = f.read()
    with open(os.path.join(task_path, "task.lean"), "r") as f:
        lean_code_template = f.read()
    return problem_description, lean_code_template


def get_unit_tests_from_taskpath(task_path: str) -> str:
    """Read unit tests from task directory."""
    with open(os.path.join(task_path, "tests.lean"), "r") as f:
        unit_tests = f.read()
    return unit_tests


def get_task_lean_template_from_taskpath(task_path: str) -> str:
    """Read Lean code template from task directory."""
    with open(os.path.join(task_path, "task.lean"), "r") as f:
        task_lean_template = f.read()
    return task_lean_template


# ============================================================================
# GLOBAL INSTANCES AND CLEANUP
# ============================================================================

# Global instances for state persistence
global_cache_manager = FastCacheManager()


def cleanup_and_save():
    """Cleanup function to save caches"""
    try:
        global_cache_manager.save_caches()
        print("‚úÖ Caches saved successfully")
    except Exception as e:
        print(f"‚ùå Cleanup failed: {e}")


# Register cleanup function
import atexit
atexit.register(cleanup_and_save)


# ============================================================================
# PERFORMANCE TESTING AND BENCHMARKING
# ============================================================================

def benchmark_workflow(task_ids: List[str], iterations: int = 1) -> Dict[str, Any]:
    """Benchmark the workflow against multiple tasks"""
    print(f"üèÅ Starting benchmark of {len(task_ids)} tasks with {iterations} iterations each")
    
    benchmark_results = {
        "task_results": {},
        "overall_stats": {},
        "performance_breakdown": {}
    }
    
    total_start_time = time.time()
    
    for task_id in task_ids:
        task_results = []
        
        for iteration in range(iterations):
            print(f"\nüìã Benchmarking {task_id} - Iteration {iteration + 1}/{iterations}")
            
            try:
                # Load task
                task_path = f"tasks/{task_id}"
                problem_description, task_lean_code = get_problem_and_code_from_taskpath(task_path)
                
                # Run workflow
                start_time = time.time()
                result = main_workflow(problem_description, task_lean_code)
                end_time = time.time()
                
                # Record results
                task_results.append({
                    "success": result["code"] != "sorry" and result["proof"] != "sorry",
                    "runtime": end_time - start_time,
                    "code_length": len(result["code"]),
                    "proof_length": len(result["proof"])
                })
                
            except Exception as e:
                print(f"‚ùå Benchmark error for {task_id}: {e}")
                task_results.append({
                    "success": False,
                    "runtime": 0,
                    "error": str(e)
                })
        
        benchmark_results["task_results"][task_id] = task_results
    
    total_end_time = time.time()
    
    # Calculate overall statistics
    all_results = [result for results in benchmark_results["task_results"].values() 
                   for result in results if "error" not in result]
    
    if all_results:
        benchmark_results["overall_stats"] = {
            "total_runtime": total_end_time - total_start_time,
            "average_runtime": sum(r["runtime"] for r in all_results) / len(all_results),
            "success_rate": sum(1 for r in all_results if r["success"]) / len(all_results),
            "total_tasks": len(task_ids) * iterations,
            "successful_tasks": sum(1 for r in all_results if r["success"])
        }
    
    # Print benchmark summary
    print(f"\nüèÜ BENCHMARK SUMMARY:")
    print(f"   Total Tasks: {len(task_ids) * iterations}")
    print(f"   Success Rate: {benchmark_results['overall_stats'].get('success_rate', 0) * 100:.1f}%")
    print(f"   Average Runtime: {benchmark_results['overall_stats'].get('average_runtime', 0):.2f}s")
    print(f"   Total Time: {benchmark_results['overall_stats'].get('total_runtime', 0):.2f}s")
    
    return benchmark_results


# ============================================================================
# CACHE KEY DEBUGGING UTILITY
# ============================================================================

def debug_cache_keys():
    """Debug utility to show cache key generation"""
    cache_manager = FastCacheManager()
    
    # Test cache key generation with some examples
    test_cases = [
        ("identity function", "ident", "identity", 0.1),
        ("multiply function", "multiply_by_three", "arithmetic", 0.2),
        ("minimum function", "minimum", "comparison", 0.6),
    ]
    
    print("üîç CACHE KEY DEBUG:")
    print("=" * 50)
    
    for desc, func_name, prob_type, complexity in test_cases:
        key = cache_manager.generate_cache_key(desc, func_name, prob_type, complexity)
        print(f"Description: {desc}")
        print(f"Function: {func_name}")
        print(f"Type: {prob_type}")
        print(f"Complexity: {complexity}")
        print(f"Cache Key: {key}")
        print("-" * 30)
def debug_cache_status():
    """Debug current cache status"""
    cache_manager = FastCacheManager()
    print(f"\nüîç CACHE DEBUG:")
    print(f"   Solutions cached: {len(cache_manager.solution_cache)}")
    print(f"   Cache keys: {list(cache_manager.solution_cache.keys())}")
    for key, data in cache_manager.solution_cache.items():
        print(f"   {key}: {data['solution']['code'][:20]}...")

if __name__ == "__main__":
    # Example usage and testing
    print("üß™ Testing Performance Optimized RAG-CAG System")
    
    # Debug cache keys
    debug_cache_keys()
    
    # Test with a simple task
    test_description = "Write a function that returns the identity of a natural number"
    test_code = """
import Mathlib
import Aesop

def ident (x : Nat) : Nat :=
  {{code}}

def ident_spec (x : Nat) (result: Nat) : Prop :=
  result = x

theorem ident_spec_satisfied (x : Nat) :
  ident_spec x (ident x) := by
  {{proof}}
"""
    
    result = main_workflow(test_description, test_code)
    print(f"\nüéØ Test Result:")
    print(f"   Code: {result['code']}")
    print(f"   Proof: {result['proof']}")
    
    # Show final performance insights
    print(profiler.get_performance_insights())